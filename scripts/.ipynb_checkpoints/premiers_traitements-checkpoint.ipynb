{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6822a9c1-71ec-476b-8bf0-22a3df70cc8d",
   "metadata": {},
   "source": [
    "# Réaliser quelques traitements avec Python\n",
    "\n",
    "Dans ce notebook, on explore différents \"outils\" permettant de faire des traitements simples comme \"pseudonymiser\" les entretiens, compter les occurrences d'un mot ou faire ressortir les termes spécifiques d'un entretien.\n",
    "\n",
    "Certains de ces traitements peuvent être réalisés avec les modules de base de Python, d'autres font appel à des modules spécialisés dans le traitement automatique des langues comme les modules `Spacy` ou `Scikit Learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f580e4ec-80a2-405f-9415-84d88528dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypandoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cac710-c9d2-4c91-a872-4fdbbb5f07cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Module pour utiliser les \"regex\", i.e. les expressions régulières\n",
    "\n",
    "import pandas as pd # module pour manipuler des \"dataframes\". \n",
    "import csv # Outil pour lire et écrire des fichiers csv\n",
    "import numpy as nmp\n",
    "\n",
    "\n",
    "import pypandoc\n",
    "\n",
    "import fileinput\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95043c3-6f83-4213-b67c-e6f09fdc781a",
   "metadata": {},
   "source": [
    "## Chargement des données\n",
    "\n",
    "On ouvre les fichiers csv contenant les retranscriptions avec `pandas`. Pour cela nous avons besoin de connaitre l'adress (ou chemin) du fichier et d'utiliser la fonction pd.read_csv(adresse du fichier, sep =\",\"). L'argument \"sep\" correspond au séparateur utiliser pour marquer les colonnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f556d425-56a0-42de-a8ca-152a36aeff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_transcript = \"../records/grazia_borrini_07-06-18.csv\" # Adresse du fichier csv\n",
    "gb = pd.read_csv(gb_transcript, sep=\",\") #je lis le fichier avec pandas et je \"stocke\" les données dans un objet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fafb31-3af8-45ff-b5a4-e9b0dce1cfa0",
   "metadata": {},
   "source": [
    "**Exercice**\n",
    "\n",
    "Ouvrez le fichier \"Marie_Roue_29-03-18.csv\" avec pandas. Vous utiliserez \"mr\" (pour Marie Roué) pour nommer l'objet contenant le dataframe (c'est-à-dire les données du fichier csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d80d83-65ff-48fb-b00e-2901a4673539",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_transcript = \"../records/Marie_Roue_29-03-18.csv\"\n",
    "#Ci-dessous lisez le fichier avec pandas et stockez le dans un objet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73384117-1b41-4058-9c7b-14193fe69ffc",
   "metadata": {},
   "source": [
    "# Regrouper les segments en blocs\n",
    "\n",
    "Whisper découpe l'entretient en segment de plus ou moins 10 secondes. Pour lire l'entretien, il peut toutefois être plus facile de regrouper les segments liés à une même question par exemple. Le regroupement facilitera également le calcul du \"TF-IDF\" que l'on verra plus loin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f855aa-d391-490a-ac9d-1df53c981e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bloc(data, column, new_dataframe = False) : #fonction pour regrouper les segments d'un même bloc (prise de parole d'un locuteur)\n",
    "    \"\"\"Groupe les segments d'une retranscription whisper en bloc.\n",
    "\n",
    "    ------\n",
    "    data : le dataframe contenant la retranscription. Le dataframe doit avoir à minima une colonne \"segment\"\n",
    "    column : la colonne à partir de laquelle  le regroupement est effectué.\n",
    "    new_dataframe : Si faux (argument par défaut), la fonction crée une nouvelle colonne au dataframe (dans lequel une ligne est égal à un segment). Si vrai, la fonction retourne un nouveau dataframe dans lequel une ligne est égal un bloc\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_bloc_id = {}\n",
    "    for n, speaker in enumerate(data[column]):\n",
    "        if n == 0:\n",
    "            bloc_id = n\n",
    "        else :\n",
    "            if speaker == data[column].iloc[n-1]:\n",
    "                bloc_id = int(dict_bloc_id[n-1].replace(\"bloc_\",\"\"))\n",
    "            else:\n",
    "                bloc_id = int(dict_bloc_id[n-1].replace(\"bloc_\",\"\"))+1\n",
    "        dict_bloc_id[n] = \"bloc_\"+str(bloc_id)\n",
    "    data[\"bloc_id\"] = data.index.map(dict_bloc_id.get)\n",
    "    data[\"bloc_text\"] = data.groupby(['bloc_id'])['text'].transform(lambda x : ' '.join(x))\n",
    "    data[\"start_bloc\"] = data.groupby(['bloc_id'])['start'].transform(\"min\")\n",
    "    data[\"end_bloc\"] = data.groupby(['bloc_id'])['end'].transform(\"min\")\n",
    "    data[\"start_bloc_ms\"] = data.groupby(['bloc_id'])['start_in_ms'].transform(\"min\")\n",
    "    data[\"end_bloc_ms\"] = data.groupby(['bloc_id'])['end_in_ms'].transform(\"min\")\n",
    "    if new_dataframe==True:\n",
    "        data = data.drop(columns=[\"text\", \"id\",\"start\", \"end\", \"start_in_ms\", \"end_in_ms\"]).drop_duplicates()\n",
    "    return dict_bloc_id, data\n",
    "\n",
    "\n",
    "def write_bloc_in_list(data, column_of_text = \"text\", segment_tag=False, segment_id=None):\n",
    "    \"\"\"\n",
    "    Ecrit les blocs obtenu avec la fonction \"get_bloc\" dans une liste.\n",
    "\n",
    "    ------\n",
    "    data: le dataframe contenant la retranscription avec les segments regroupés par bloc.\n",
    "    column_of_text : le nom de la colonne contenant le text.\n",
    "    segment_tag : Boolean (Faux par défaut). Si vrai et que le dataframe contient une colonne segment, la fonction ajout un tag sous la forme \"<seg_id>segment</seg_id>\" pour marquer le début et la fin de chaque segment d'un même bloc.\n",
    "    segment_id : non de la colonne contenant les idenfitifiants des segments\n",
    "\n",
    "    Retourne une liste des blocs (utile pour le calcul du tf-idf ensuite)\n",
    "    \"\"\"\n",
    "    \n",
    "    list_bloc_text = []\n",
    "    #dict_speaker = {}\n",
    "    for n, b in enumerate(data.bloc_id.unique()):\n",
    "        data_tmp = data.loc[data.bloc_id == b]\n",
    "        if segment_tag == True:\n",
    "            new_text = \"\"\n",
    "            try:\n",
    "                for m, seg in enumerate(data_tmp[segment_id]):\n",
    "                    new_text = new_text + \"<\" + seg + \">\" + data_tmp[column_of_text].iloc[m]  + \"</\" + seg + \">\" #décommenter la ligne ci-dessous si on souhaite mettre des balises indiquant le début et la fin des segments que l'on concatène\n",
    "            except:\n",
    "                raise\n",
    "        else:\n",
    "            new_text = data_tmp[column_of_text].iloc[0] \n",
    "        list_bloc_text.append({\"bloc_id\":b, \"speaker\": data_tmp.speaker.unique()[0], \"bloc_text\" : new_text, \"start_bloc\": data_tmp.start_bloc.iloc[0], \"end_bloc\": data_tmp.end_bloc.iloc[0]})\n",
    "    return list_bloc_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78992f85-dad2-4082-a50c-e41ef9c58058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_bloc_id, gb1 = get_bloc(data=gb, column=\"speaker\", new_dataframe=False)\n",
    "gb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1a9ca-47cc-4183-bd1e-3a720220d427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_bloc_in_list(data=gb1, column_of_text = \"bloc_text\", segment_tag=False, segment_id=\"id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95394ddd-650e-42d1-a56e-10e70ddde36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_bloc_id, gb1 = get_bloc(data=gb, column=\"speaker\", new_dataframe=False)\n",
    "list_bloc_text = write_bloc_in_list(data=gb1, column_of_text = \"bloc_text\", segment_tag=True, segment_id=\"id\")\n",
    "with open(\"../records/2026-02-05_grazia_borrini_in_block.txt\" , 'w', newline='') as txt:\n",
    "    for row in list_bloc_text:\n",
    "        start = row[\"start_bloc\"]\n",
    "        end = row[\"end_bloc\"]\n",
    "        speaker = row[\"speaker\"]\n",
    "        bloc_text = row[\"bloc_text\"]\n",
    "        txt.write(f\"{row[\"bloc_id\"]}\\n\")\n",
    "        txt.write(f\"{start} --> {end}\\n\")\n",
    "        txt.write(f\"{speaker} : {bloc_text}\\n\\n\")\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a0c19-36c7-4fc9-a307-c9f6cffcdcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dataframe = [gb, mr]\n",
    "list_of_path = [gb_transcript, mr_transcript]\n",
    "for n, df in enumerate([gb, mr]):\n",
    "    dict_bloc_id, df1 = get_bloc(data=df, column=\"speaker\", new_dataframe=False)\n",
    "    list_bloc_text = write_bloc_in_list(data=df1, column_of_text = \"bloc_text\", segment_tag=False, segment_id=None)\n",
    "    with open(f\"{list_of_path[n]}_in_bloc.txt\" , 'w', newline='') as txt:\n",
    "        for row in list_bloc_text:\n",
    "            start = row[\"start_bloc\"]\n",
    "            end = row[\"end_bloc\"]\n",
    "            speaker = row[\"speaker\"]\n",
    "            bloc_text = row[\"bloc_text\"]\n",
    "            txt.write(f\"{row[\"bloc_id\"]}\\n\")\n",
    "            txt.write(f\"{start} --> {end}\\n\")\n",
    "            txt.write(f\"{speaker} : {bloc_text}\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69fd115-9ee4-4c16-a39c-fef33a607749",
   "metadata": {},
   "source": [
    "# Pseudonomyser et anonymiser les locuteurs\n",
    "\n",
    "Quand on travaille avec des entretiens, il est souvent nécessaire de les pseudonomyser. On peut pour cela utiliser l'outil \"find and replace\" de son loogiciel de traitement de texte. On peut également le faire avec python, notamment lorsqu'on a de nombreux noms de personnes ou de lieux à remplacer.\n",
    "\n",
    "C'est ce que propose de faire la fonction ci-dessous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5ebf04-45bb-4816-a581-311667a5a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudonymiser(original_name, new_name, file, column = None, replace_file=False):\n",
    "    \"\"\"\n",
    "    Pseudonymise un fichier\n",
    "    ---------------\n",
    "\n",
    "    original_name : 'string'.\n",
    "    new_name : 'string'. The pseudonym\n",
    "    file : path of file to pseudonymise. It could be a csv, srt or txt file.\n",
    "    column : only if \"file\" is a csv. Name of the column to pseudonymise\n",
    "    replace_file: boolean. If false, it create a new file. Else it erases the given file.\n",
    "    \"\"\"\n",
    "    \n",
    "    if file.split(\".\")[-1] == \"csv\":\n",
    "        with open(file, \"r\") as csvf:\n",
    "            reader2 = csv.DictReader(csvf)\n",
    "            old_row = []\n",
    "            line_to_override={}\n",
    "            for n, r in enumerate(reader2):\n",
    "                old_row.append(r)\n",
    "                pseudo = r[column].replace(original_name, new_name)\n",
    "                r[f\"pseudo_{column}\"] = pseudo\n",
    "                line_to_override[n] = r\n",
    "        if replace_file == False:\n",
    "            new_file = file.replace(\".csv\", \"pseudonymise.csv\")\n",
    "            with open(new_file, 'w', newline='') as csvfile:\n",
    "                fieldnames = [key for key, value in line_to_override[0].items()]\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                for line, row in enumerate(old_row):\n",
    "                    data = line_to_override.get(line, row)\n",
    "                    #print(data)\n",
    "                    writer.writerow(data)\n",
    "            return new_file\n",
    "        else:\n",
    "            with open(file, 'w', newline='') as csvfile:\n",
    "                fieldnames = [key for key, value in line_to_override[0].items()]\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                for line, row in enumerate(old_row):\n",
    "                    data = line_to_override.get(line, row)\n",
    "                    #print(data)\n",
    "                    writer.writerow(data)\n",
    "            return file\n",
    "    else:\n",
    "        if replace_file == False:\n",
    "            find_ext = file.split(\".\")[-1]\n",
    "            new_file = file.replace(f\".{find_ext}\", f\"pseudonymise.{find_ext}\")\n",
    "            with open(file, 'r', encoding='utf-8-sig') as txt:\n",
    "                with open(new_file, 'w', encoding='utf-8') as txt2:\n",
    "                    for line in txt:\n",
    "                        new_line = re.sub(original_name, new_name, line)\n",
    "                        txt2.write(new_line)\n",
    "            return new_file\n",
    "        else:\n",
    "            for line in fileinput.input(file, inplace = 1): \n",
    "                print(line.replace(original_name, new_name))\n",
    "            return file\n",
    "            \n",
    "                            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105b46d-a231-45a4-a08d-2c3e58befc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_replace= {\"Speaker 1\": \"Chercheur\", \"Speaker 0\": \"Enquêtée\"}\n",
    "\n",
    "n = 0\n",
    "for nom, new_name in name_to_replace.items():\n",
    "    n +=1\n",
    "    if n == 1:\n",
    "        nom_fichier = pseudonymiser(nom, new_name, file = \"../records/2026-02-05_grazia_borrini_in_block.txt\", column= \"speaker\", replace_file=False)\n",
    "    else:\n",
    "        pseudonymiser(nom, new_name, file = nom_fichier, column= \"pseudo_speaker\", replace_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bccaa2-0e8d-42a5-b19a-0c75ccac2713",
   "metadata": {},
   "source": [
    "# Compter un ou plusieurs mots\n",
    "\n",
    "Nous créons maintenant un compteur de mots (équivalent à la fonction \"find\" dans word). On utiliser pour cela des expressions régulières (Regex). Les regex permettent par exemple de retrouver tous les mots contenant une même racine (ou terminaisons). Dans l'exemple ci-dessous, nous allons rechercher tout les mots qui contiennent la racine \"participat\" : participation, participatif, participative, participatory, etc.\n",
    "\n",
    "Nous allons faire la recherche sur deux types de fichier : un fichier texte (.txt) et un fichier csv.\n",
    "\n",
    "## Trouver les occurences de l'adjectif \"participative\" dans l'entretien de Grazia Borrini\n",
    "\n",
    "Nous commençons par explorer l'entretien de Grazia Borrini en utilisant le fichier texte obtenu après avoir regroupés les segments en bloc. Ce fichier s'appelle `2026-02-05_grazia_borrini_in_block.txt`.\n",
    "\n",
    "Pour cela, j'ouvre le fichier et je \"mets\" son contenu dans l'objet \"texte\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8572b-eb39-48f0-8267-cbcf15c7b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../records/2026-02-05_grazia_borrini_in_block.txt\",\"r\") as f:\n",
    "    texte = f.read()\n",
    "\n",
    "len(texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9f2a1e-8952-4f9a-b6c3-b180d21a29c6",
   "metadata": {},
   "source": [
    "Une fois l'objet `texte` créé, on peut effectuer différents décomptes :\n",
    "\n",
    "* Si je veux connaître le nombre de caractères, j'exécute la commande `len(texte)` (len pour length).\n",
    "* Si je veux connaître le nombre de mots/chaînes de caractères : `len(texte.split())`. La fonction `.split()` découpe le texte à chaque fois qu'elle trouve une espace.\n",
    "* Si je veux connaître le nombre d'occurrence d'un mot quelconque : `texte.count(\"le mot que je cherche\")`.\n",
    "\n",
    "Essayez ces différentes commandes dans la cellule ci-dessous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ec5a3-e760-4639-97f8-0af044966051",
   "metadata": {},
   "outputs": [],
   "source": [
    "texte.count(\"participative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b4e6ef-68a1-4ba9-b14d-ccf6ba165d7a",
   "metadata": {},
   "source": [
    "Bien sûr, il est possible d'inscrire les résultats dans des objets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35643fd7-66aa-4dd7-8058-88081d57ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_caractere = len(texte)\n",
    "nb_string = len(texte.split())\n",
    "occ_participat = texte.count(\"participative\")\n",
    "\n",
    "print(\"Nombre de caractères \",nb_caractere) # affiche le nombre de chaînes de caractère\n",
    "print(\"Longueur du texte \", nb_string) # affiche le nombre de chaînes de caractère\n",
    "print(\"Occurence du terme 'participative' \", occ_participat) # compte le nombre de fois que participative apparaît"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4003b2-842a-496c-b7fa-4fb0c9780ba6",
   "metadata": {},
   "source": [
    "Combien de fois le terme \"participative\" apparaît dans l'entretien de Grazia Borrini ? Quel est le problème avec cette clé de recherche ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3203f2b5-8f64-4492-9e01-93ab666692f1",
   "metadata": {},
   "source": [
    "C'est en ce sens que les regex sont utiles, que ce soit dans le cadre des traitements que nous faisons, mais aussi dans tous les cas où vous faites une recherche sur word, le web, etc. Elles permettent de préciser la recherche en l'élargissant ou en la spécifiant. Par exemple `texte.count()` compte les formes singulières et plurielles. Mais parfois, il peut intéressant de compter le nombre de fois qu'un mot est mis au singulier ou mis au pluriel.\n",
    "\n",
    "```\n",
    "Il existe de nombreuses ressources sur le web pour comprendre le fonctionnement des regex et le rôle des différentes \"balises\". Vous pouvez en particulier consulter le tutoriel sur le site W3school : [https://www.w3schools.com/python/python_regex.asp](https://www.w3schools.com/python/python_regex.asp)```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f08ff50-7af3-4316-a588-c129a8ef947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Il a une fois participative dans sciences participatives.\"\n",
    "\n",
    "test.count(\"participative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fb5f15-7017-482f-8e8b-be336a853adc",
   "metadata": {},
   "source": [
    "La fonction ci-dessous est similaire à la fonction test.count(mot_a_chercher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9f8a0b-1979-41bb-a8b2-6586c10eed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"participative\", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13026665-260b-4366-91cc-66aa0895727b",
   "metadata": {},
   "source": [
    "* Si on ne cherche que les formes au singulier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42271f-a995-443c-809e-0d3d3630f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"participative\\b\", test) # le \"\\b\" indique que l'on recherche toutes les chaines de caractères finissant par \"participative\", c'est-à-dire le mot \"participative\" suivi d'un espace ou d'un signe de ponctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3fab2b-100f-499c-90ef-9276cd99e6bd",
   "metadata": {},
   "source": [
    "* Si on ne cherche que les formes au pluriel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61915e2e-4af8-4d2f-aae3-3b926bf92eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"participatives\", test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c84ffb-f31f-4869-bdf8-2ded8a1d260b",
   "metadata": {},
   "source": [
    "**OU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22ba21-84ab-49b5-8711-72cd05dd640b",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"participatives\\b\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3890e1-9531-4ee3-945f-314112011aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"participative\\w\\b\", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94e3077-afb1-4226-bce9-709d5434c178",
   "metadata": {},
   "source": [
    "* Si on cherche les deux formes, mais que l'on souhaite les distinguer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af7b167-414e-445f-8dbe-d48a7fda3027",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"participatives?\", test) # Le point d'interrogation signifie qu'on cherche toutes les occurrence de participative contenant un \"s\" ou aucun."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b819d025-2508-47d2-b26b-0f842da58479",
   "metadata": {},
   "source": [
    "**OU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33b542-1f19-48e3-ad4d-8e4e0a62f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"participatives*\\b\", test) # L'astérisque signifie qu'on cherche toutes les occurrence de participative contenant aucun \"s\" ou plus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9161e098-fef3-4bfa-8cd7-7bd9723030da",
   "metadata": {},
   "source": [
    "**OU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca25d0d-50b1-4fca-b5ac-b2459899bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r\"participative.\", test) #La balise \".\\b\" signifie qu'on cherche toutes les occurrence de participative suivi de n'importe quel caractère en bout de chaîne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d960fff8-6062-495d-a89c-faed8c54249a",
   "metadata": {},
   "source": [
    "Dans la cellule ci-dessous, on recherche fois toutes les fois qu'apparaît la racine \"participat\" quelque soit la terminaison (participation, participatif, participatory, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef923a41-3414-485f-81a3-b2dfd1260fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_with_participat = re.findall(r\"participat\\w*\", texte.lower()) \n",
    "print(\"Le nombre de mots contenant la racine 'participat' est : \", len(words_with_participat))\n",
    "print(\"Voici la liste des différentes formes retrouvées : \", set(words_with_participat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fae358-3675-468d-beb7-506c92b5a5a0",
   "metadata": {},
   "source": [
    "En fait, la fonction `re.findall()` \"retourne\" la liste de toutes les occurrences des termes commençant par \"participat\". Si on veut la liste des différentes formes sans les doublons, on écrit l'objet contenant la liste à l'intérieur de la fonction `set()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e251f4d0-6cfe-474c-a883-a804f1b9ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"La liste des occurrences de 'participat' : \", words_with_participat[0:10])\n",
    "print(\"Voici la liste des différentes formes retrouvées sans les doublons : \", set(words_with_participat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f17a4c7-cc72-48cc-a549-858550eee56f",
   "metadata": {},
   "source": [
    "### Exercice\n",
    "\n",
    "Pour chaque terme contenu dans la liste `words_with_participat`, comptez le nombre d'occurrences dans les entretiens de Grazia Borrini, puis dans celui de Marie Roué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4184d0-a9ad-427d-b4c5-9065d68e448a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2d11539-748f-400d-a247-ba020f965d4e",
   "metadata": {},
   "source": [
    "* Trouver une regex pour identifier les expressions contenant la racine \"participat\" comme \"science participative\" ou \"démocratie participative\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81297263-cc98-4b1c-95e1-103be24a31bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfb2545f-47bd-4d6f-92d7-5e3053926c24",
   "metadata": {},
   "source": [
    "### Mettre en gras les mots recherchés\n",
    "\n",
    "Maintenant que nous avons identifiés les termes commençant par \"participat\", il peut être intéressant des les \"taguer\" dans le texte pour les repérer plus facilement à la lecture des entretiens. Ici, on choisit de les signaler en les encadrant de deux astérisques, mais on peut aussi utiliser des balises html, xml, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04711b8f-7e9e-40ca-9da7-a55fe8ce7887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"../records/2026-02-05_grazia_borrini_in_block.txt\",\"r\") as f:\n",
    "    texte = f.readlines()\n",
    "\n",
    "for s in texte:\n",
    "    if re.search(r'participat\\w+', s.lower()):\n",
    "        for w in set(re.findall(r'participat\\w+', s.lower())):\n",
    "            s = re.sub(w, f'**{w}**', s.lower())\n",
    "        #print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79ad2a4-e2ed-488c-b967-82e8a2d1fc54",
   "metadata": {},
   "source": [
    "Si on veut enregistrer le résultat dans un fichier .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d69a6-f4ff-4e49-a080-4988f06324a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../records/2026-02-05_grazia_borrini_in_block.txt\" , 'w', newline='') as txt:\n",
    "    for s in texte: \n",
    "        txt.write(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f7274e-ff95-4751-83c5-1d98a65b1e81",
   "metadata": {},
   "source": [
    "La cellule ci-dessous permet de faire la même recherche dans des données présentées dataframe (tableur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96251f04-af25-4c77-8509-c73da52410f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_n_word = {}\n",
    "dict_word = {}\n",
    "for n, seg in enumerate(gb.id):\n",
    "    row = gb.iloc[n]\n",
    "    n_word = len(re.findall(r'participat\\w+', row[\"text\"].lower()))\n",
    "    word = [x for x in set(re.findall(r'participat\\w+', row[\"text\"].lower()))]\n",
    "    if n_word > 0:\n",
    "        \n",
    "        dict_word[seg]= \"|\".join(word)\n",
    "        dict_n_word[seg] = n_word\n",
    "\n",
    "dict_word\n",
    "gb[\"partipat\"] = gb.id.map(dict_word.get)\n",
    "gb[\"nb_partipat\"] = gb.id.map(dict_n_word.get)\n",
    "gb.loc[gb.nb_partipat>0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df13ab8b-fa8d-4d4d-a0a9-f6a0dc08a237",
   "metadata": {},
   "source": [
    "Le problème de cette méthode est qu'il faut connaître a priori les termes que l'on souhaite rechercher. Nous allons maintenant voir comment faire ressortir les termes spécifiques de l'entretien."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56301f79-2755-48e3-9cb6-e7cedc1eee68",
   "metadata": {},
   "source": [
    "# Identifier les termes \"spécifiques\" d'un entretien"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b847f9e-ce04-43b9-bc63-1cb75f1d02ff",
   "metadata": {},
   "source": [
    "## Le Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "Le \"Term Frequency-Inverse Document Frequency\" (TF-IDF) est un indice statistique utilisé dans la recherche d'information et l'exploration de documents. Il permet de \"mesurer\" l'importance d'un terme dans un document comparativement à un ensemble de documents. Au sens du \"TF-IDF\", un mot est important dans un document lorsqu'il apparait plus fréquement que dans le reste du corpus. Par exemple, si dans un entretien l'expression \"savoirs locaux\" apparaît 10 fois alors que dans les autres il n'apparaît jamais, on peut supposer que c'est une expression \"importante\" pour la personne qui la cite.\n",
    "\n",
    "Les éléments qui vont suivre permettent de calculer ce TF-iDF. Nous allons d'abord voir comment le calculer 'from scratch', afin de comprendre les différents élements qui entre dans le calcul de l'indice au cas où voudriez le faire dans excel par exemple. Puis, nous utiliserons des modules pythons qui permettent de le faire de façon plus \"concise\".\n",
    "\n",
    "Le calcul du TF-IDF suppose au préalable de \"nettoyer\" les textes : le passer en petite casse, retirer les signes de ponctuation et les \"stop words\" (mots vides en français). Les \"stop words\" sont des mots tellement communs qu'ils n'apportent théoriquement aucune information sur le contenu du texte. Les articles (la, le, les, de, du, des, etc.), les auxilliaires (être, avoir) sont généralement considérés comme des stop words. \n",
    "\n",
    "Plusieurs modules sous Python proposent des listes de \"stop words\". Je vous propose d'utiliser les modules \"Spacy\" et \"Nltk\" conçues spécialement pour le traitement automatique des langues.\n",
    "\n",
    "### Installer et charger Spacy\n",
    "\n",
    "Spacy n'est pas encore installé. Pour cela nous ouvrons une cellule de code et écrivons `!pip install spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b8f259-0845-4b6f-ae3b-65d064c1ae19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60fc46e-8bfa-4290-841b-88b0dcabaf57",
   "metadata": {},
   "source": [
    "Une fois spacy installé, nous allons télécharger le modèle de langue dont on n'a besoin. Dans le cas des entretiens utilisés pour l'atelier, on prend le modèle français. Mais il existe d'autres langues : l'anglais bien sûr, mais aussi l'allemand, le chinois, le polonais, etc. \n",
    "\n",
    "Vous trouverez les modèles de langue existant ici  : [https://spacy.io/models](https://spacy.io/models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08e46b4-d721-4889-b486-0bc034d9db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download fr_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d330bac-37e5-4133-bf3e-803184e48a4e",
   "metadata": {},
   "source": [
    "### Les stop words proposés par Spacy\n",
    "\n",
    "Enfin, on peut importer le module spacy, charger le modèle et imprimer la liste des stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b6186-8216-4d87-9904-e7a8a0109a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "print(f\"Il y a {len(nlp.Defaults.stop_words)} termes dans la liste des stopwords de Spacy\")\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101cefe8-0fe3-417d-b3b3-3389fa349390",
   "metadata": {},
   "source": [
    "### Les stop words proposés par le module nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d43295-7e4f-4dca-971b-8f0b9d82ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e253a9e8-c6b5-46cc-9ec4-aacfb0ecfc19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords.words(\"french\")\n",
    "print(f\"Il y a {len(stopwords.words(\"french\"))} termes dans la liste des stopwords de Spacy\")\n",
    "print(stopwords.words(\"french\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d311ab-8b4e-4b69-b579-212a4acc8d24",
   "metadata": {},
   "source": [
    "### Nettoyage du texte\n",
    "\n",
    "Nous allons \"nettoyer\" le texte de l'entretien de Grazia Borrini et le \"tokenizer\" (i.e. diviser en mot). Nous allons par ailleurs travailler au niveau des blocs pour voir quels sont les thèmes qui émergent au fil de l'entretien ou ceux qui reviennent. Par défaut, aucun stopword n'est retiré.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af07db-d183-44e6-8820-0a3e48d79971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text, lower_case = True, remove_stopword=None):\n",
    "    if lower_case == True:\n",
    "        text = text.lower()\n",
    "    else:\n",
    "        pass\n",
    "    text = re.sub(r\"http\\S+\", \"\", text) # remove urls\n",
    "    text = re.sub(\"’\", \"'\",text) #replace \"’\" by \"'\"\n",
    "    text = re.sub(\"'\", \"' \",text) #replace \"’\" by \"'\"\n",
    "    text = \" \".join(re.sub(\"[^a-zA-Zàâäéèêëïîôöùûüÿçß]+\", \" \", text).lower().split()) #remove hashtag, arobase, HTML character\n",
    "    text = re.sub(r\"\\b[a-zA-Z].\\b\", \"\", text)\n",
    "    if remove_stopword == None:\n",
    "        split_text = text.split() # \n",
    "    else:\n",
    "        split_text = [x for x in text.split() if x not in remove_stopword]\n",
    "        text = ' '.join(split_text)\n",
    "    \n",
    "    return text, split_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c9913-56e6-42a0-b185-0675d1aaba90",
   "metadata": {},
   "source": [
    "La fonction `cleaner()` prend en entrée un texte (un string en langage informatique). L'argument \"remove_stopword\" prend une liste de termes que l'on souhaite retirer pour l'analyse. Elle retourne ensuite deux objets : \n",
    "* la phrase \"nettoyée\", c'est-à-dire sans ponctuation, sans caractères spéciaux et en petite casse (on peut préciser ̀`lower_case = False` si on veut garder les majuscules)\n",
    "* la phrase tokenizée\n",
    "\n",
    "**Essayons-là!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefa8166-d2da-46e8-9267-8d25ebc9b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"Le chat mange une souris. La souris est mangée par un chat. La souris que le chat a mangée! #souris #chat. C'est le chat qui mange la souris.\"\n",
    "\n",
    "cleaned_phrase, token_phrase = cleaner(phrase)\n",
    "print(\"La phrase nettoyée : \", cleaned_phrase)\n",
    "print(\"La phrase tokenizée : \", token_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158470a6-f01c-4938-ba46-087748edd9c9",
   "metadata": {},
   "source": [
    "La même phrase avec les \"stop_words en moins\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08201d09-c632-4143-b1d0-c94ad74711e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_stopwords = stopwords.words(\"french\") # on met les stop words de nltk dans une liste\n",
    "cleaned_phrase, token_phrase = cleaner(phrase, remove_stopword=fr_stopwords)\n",
    "print(\"La phrase nettoyée : \", cleaned_phrase)\n",
    "print(\"La phrase tokenizée : \", token_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ff2df-1d7d-4c41-93e1-0b5e467d532c",
   "metadata": {},
   "source": [
    "### Exercice\n",
    "\n",
    "Utiliser la même fonction `cleaner` en utilisant cette fois la liste des stopwords proposée par Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1931f78f-a3fa-4086-802b-fe4fbfbd6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire le code dans cette cellule\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f248343e-ca45-49db-9154-e400c0d74534",
   "metadata": {},
   "source": [
    "Nous allons maintenant répéter la fonction de nettoyage sur chacun des segments de l'entretien de Grazia Borrinin en utilisant le dataframe `gb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f5bba-c7f3-4121-ae67-f1b0abae7dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_stopwords = list(nlp.Defaults.stop_words)#stopwords.words(\"french\")\n",
    "\n",
    "dict_token = {} #dictionnaire pour \"conserver\" la liste des tokens de chaque segment\n",
    "dict_text = {} #dictionnaire pour \"conserver\" les segments nettoyés\n",
    "dict_size_token = {} #dictionnaire pour \"conserver\" le nombre de tokens par segment\n",
    "for n, seg in enumerate(gb.id): # pour chaque segment dans la colonne gb.id\n",
    "    row = gb.iloc[n] #on extrait lla ligne\n",
    "    text = row['text']#on extrait le texte\n",
    "    clean_text, split_text = cleaner(text, lower_case = True, remove_stopword=fr_stopwords)\n",
    "    dict_token[seg]=\"|\".join(split_text)\n",
    "    dict_size_token[seg] = len(split_text)\n",
    "    dict_text[seg]= clean_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a0ec76-4bee-4c79-9399-fb0d2b239a97",
   "metadata": {},
   "source": [
    "Les objets `dict_token` et `dict_text` sont des \"dictionnaires\" qui nous permettront d'ajouter la liste des token et les phrases nettoyées à notre dataframe. Pour faciliter le comptage, on va ensuite utilise la fonction `explode()` de pandas permettant de créer un nouveau dataframe avec une ligne par token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f70b5d-2834-4daa-9abe-e9e69a0cdec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb[\"cleaned_text\"] = gb.id.map(dict_text.get)\n",
    "gb[\"tokens\"] = gb.id.map(dict_token.get)\n",
    "gb[\"nb_tokens\"] = gb.id.map(dict_size_token.get)\n",
    "\n",
    "gb[\"tokens\"]= gb.tokens.str.split(\"|\")\n",
    "\n",
    "gb_explode = gb[[\"id\",\"bloc_id\",\"speaker\",\"tokens\",\"nb_tokens\"]].loc[gb.nb_tokens>0].explode(\"tokens\")\n",
    "gb_explode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f565b33e-a8ea-4f1d-aaed-eb4e2ab4f2bc",
   "metadata": {},
   "source": [
    "### Décortiquer le TF-IDF\n",
    "\n",
    "Cette partie s'inspire de l'article \"Analyse de documents avec TF-IDF\" de Matthew Lavin publié dans la revue en ligne *Programming Historian* (revue que je recommande pour ces nombreux articles didactiques).\n",
    "\n",
    "La formule mathématique pour calculer le TF-IDF est la suivante :\n",
    "\n",
    "\\begin{equation}\n",
    "Tf{\\text -}Idf_i = Tf_i \\times Idf_i\n",
    "\\end{equation}\n",
    "\n",
    "* $Tf_i$ : la fréquence du terme *i* dans le document. Selon les algorithmes, la fréquence correspond à la fréquence absolue ou à la fréquence relative (i.e. fréquence du terme *i* divisé par le nombre de tokens dans le document)\n",
    "* $Idf_i$ : la fréquence *inverse* de documents (inverse document frequency) du terme *i*. Elle correspond au *logarithme naturel* du nombre de documents analysés divisé par le nombre de documents contenant le terme *i*.\n",
    "\n",
    "$$\n",
    "Idf_i = ln\\left[{\\frac{N}{df_i}}\\right]\n",
    "$$\n",
    "\n",
    "* N : le nombre de document analysés\n",
    "* df_i : le nombre de document contenant *i*.\n",
    "\n",
    "Souvent, les algorythmes implémentés dans les modules informatiques \"normalisent\" l'idf en ajoutant $+1$ au numérateur puis au logarithme. C'est le cas du module Scikit-learn que nous allons utilisé plus bas.\n",
    "\n",
    "$$\n",
    "Idf_i = ln\\left[{\\frac{N+1}{df_i}}\\right]+1\n",
    "$$\n",
    "\n",
    "Si la formule peut impressionnée, elle n'a rien de vraiment compliqué puisque la plupart des opérations sont des additions, mutliplications et divisions. En ce qui concerne le logarithme, nous allons utilisez le module numpy.\n",
    "\n",
    "\n",
    "Un petit exemple concret :\n",
    "\n",
    "On remarque que dans un entretien le mot \"domination\" apparaît 10 fois. Ce mot apparaît par ailleurs dans 5 entretiens et notre corpus en contient 30.\n",
    "On remarque également que dans ce même entretien le mot \"famille\" apparaît 20 fois et qu'il est présent dans les 30 entretiens.\n",
    "\n",
    "|terme | $tf_i$ | $df_i$ | $N$| $tf{\\text -}idf_i$|\n",
    "|------|--------|--------|----|-----------|\n",
    "|domination| 10 | 5 | 30 | 28,25 |\n",
    "|famille | 20 | 30 | 30 | 20,66 |\n",
    "\n",
    "$$\n",
    "tf{\\text -}idf_{domination} = 10 \\times \\left[len\\left(\\frac{30+1}{5}\\right) +1\\right] \\approx 28,25 \n",
    "$$\n",
    "\n",
    "$$\n",
    "tf{\\text -}idf_{famille} = 20 \\times \\left[len\\left(\\frac{30+1}{30}\\right) +1\\right] \\approx 20,66\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b3238e-478e-4e0e-8362-e7413b9814d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as npdocument"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a148b6a-773a-42a9-8a18-94abe4a81a6f",
   "metadata": {},
   "source": [
    "La fonction `compute_tfidf` décompose l'algorithme. Pour chaque bloc, on compte le nombre de fois qu'apparaît le terme, le nombre de blocs dans lequel il est présent et le nombre de bloc total.\n",
    "On calcule par ailleurs deux tf-idf : l'un avec la fréquence absolue, l'autre avec la fréquence relative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ea742-cd1e-4a19-a556-ef517307d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(data, group_by):\n",
    "    \n",
    "    size_doc0 = data.groupby([group_by]).agg(nb_tokens=(\"tokens\",\"size\")).reset_index()\n",
    "    size_doc = size_doc0.loc[size_doc0.nb_tokens >1]\n",
    "\n",
    "    l = [x for n, x in enumerate (size_doc[group_by]) if size_doc.nb_tokens.iloc[n] > 1]\n",
    "    data = data.loc[data[group_by].isin(l)]\n",
    "    token_freq = data.groupby([group_by,\"tokens\"]).agg(term_freq=(group_by,\"size\")).reset_index()\n",
    "    #remove bloc with 0 or 1 token :\n",
    "     \n",
    "    nb_of_doc = data[group_by].nunique()\n",
    "\n",
    "    nb_doc_with_freq = data[[group_by,\"tokens\"]].drop_duplicates().groupby(\"tokens\").agg(nb_doc=(group_by,\"size\")).reset_index()\n",
    "    \n",
    "    df_tf = token_freq.merge(size_doc, on = [group_by], how = \"left\").merge(nb_doc_with_freq, on = [\"tokens\"], how = \"left\")\n",
    "    df_tf[\"tf\"]= df_tf.term_freq/df_tf.nb_tokens\n",
    "    df_tf[\"idf\"]= np.log((nb_of_seg+1)/df_tf.nb_doc)+1\n",
    "    df_tf[\"tfidf\"]= df_tf.tf*df_tf.idf\n",
    "    df_tf[\"tfidf1\"]= df_tf.term_freq*df_tf.idf\n",
    "    return df_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5b90a-33de-48cc-8afe-016815d876b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compute_tfidf(data=gb_explode, group_by=\"bloc_id\")\n",
    "#dict_tfidf\n",
    "df[\"id_bloc\"] = df.bloc_id.str.replace(\"bloc_\",\"\").astype(int)\n",
    "df = df.merge(gb_explode[[\"bloc_id\", \"speaker\"]].drop_duplicates(), on = [\"bloc_id\"], how =\"left\")\n",
    "df = df.loc[df.speaker == \"Speaker 0\"].sort_values(\"id_bloc\", ascending=True)\n",
    "\n",
    "for bloc in df.id_bloc.unique():\n",
    "    df_bloc = df.loc[df.id_bloc == bloc].sort_values(\"tfidf\", ascending=False)\n",
    "    specific_term = [x for x in df_bloc.tokens.head(5)]\n",
    "    print(f\"Les 5 termes les plus spécifique du bloc {bloc} sont : \", specific_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548ecdeb-2321-4afc-a718-250925247ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"id_bloc\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16031d7-3086-4412-b4b2-0c57c191ab33",
   "metadata": {},
   "source": [
    "## TF-IDF avec scikit learn\n",
    "\n",
    "Dans la section précédente, j'ai créé une fonction spéciale pour \"montrer\" le fonctionnement du TF-IDF en espérant que cela aide à sa compréhension. Toutefois, il existe de nombreux modules qui permettent de calculer le tf-idf uniquement en donnant une liste de document entrée. C'est le cas de Scikit-learn, un module dédié \"machine learning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f9e60-4f4c-4292-9993-ab0bcffa540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer le vectoriseur TfidfVectorizer de Scikit-Learn.  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaa8075-0539-4f48-a00d-15bf89ee737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle français de SpaCy\n",
    "nlp = spacy.load('fr_core_news_md')  # ou 'en_core_web_sm' pour l'anglais\n",
    "# Récupération et extension des stopwords par défaut de SpaCy\n",
    "spacy_stopwords = set(nlp.Defaults.stop_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385bfde-92c7-4e57-826d-8e460bf666dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour lemmatiser le texte et retirer les stopwords\n",
    "def lemmatize_and_remove_stopwords(text, nlp, stopwords, lemmatiser = False):\n",
    "    doc = nlp(text)\n",
    "    if lemmatiser == True:\n",
    "        cleaned_text = \" \".join([token.lemma_ for token in doc if token.text.lower() not in stopwords and not token.is_punct])\n",
    "    else:\n",
    "        cleaned_text = \" \".join([str(token) for token in doc if token.text.lower() not in stopwords and not token.is_punct])\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819a58a-799b-4207-946b-c584cac4e1df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gb_bloc_id, gb1 = get_bloc(data=gb, column=\"speaker\", new_dataframe=False)\n",
    "gb_bloc_text = write_bloc_in_list(data=gb1, column_of_text = \"bloc_text\", segment_tag=False, segment_id=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25816ce5-f6bd-413f-9c2d-c88bf343317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gb_bloc_text[0][\"bloc_text\"])\n",
    "print(lemmatize_and_remove_stopwords(list_bloc_text[0][\"bloc_text\"], nlp, spacy_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ad049-906e-4aaa-826d-643cae057fc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tous_documents = []\n",
    "tous_documents2 = []\n",
    "for bloc in gb_bloc_text:\n",
    "    lemmatized_an_cleaned_text = lemmatize_and_remove_stopwords(bloc[\"bloc_text\"], nlp, spacy_stopwords, lemmatiser = True)\n",
    "    tous_documents.append(lemmatized_an_cleaned_text)\n",
    "\n",
    "\n",
    "vectoriseur = TfidfVectorizer(use_idf=True)\n",
    "documents_transformes = vectoriseur.fit_transform(tous_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3b7b3c-c8ff-470b-9102-7acb54f85d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_transformes_tableau = documents_transformes.toarray()\n",
    "len(documents_transformes_tableau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b726f35-fbcc-4e29-9432-549e780ae78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "for compteur, document in enumerate(documents_transformes_tableau):\n",
    "    i+=1\n",
    "    #print(\"#####\\n\",document)\n",
    "    # construire un objet de la classe DataFrame\n",
    "    tf_idf_tuples = list(zip(vectoriseur.get_feature_names_out(), document))\n",
    "    df0 = pd.DataFrame.from_records(tf_idf_tuples, columns=['terme', 'tfidf']).sort_values(by='tfidf', ascending=False).reset_index(drop=True)\n",
    "    df0 = df0.loc[df0.tfidf > 0]\n",
    "    df0['bloc_id'] = str(compteur)\n",
    "    if i == 0:\n",
    "        df = df0.copy()\n",
    "    else:\n",
    "        df = pd.concat([df, df0])\n",
    "\n",
    "for bloc in df.bloc_id.unique():\n",
    "    df_bloc = df.loc[df.bloc_id == bloc].sort_values(\"tfidf\", ascending=False)\n",
    "    specific_term = [x for x in df_bloc.terme.head(5)]\n",
    "    print(f\"Les 5 termes les plus spécifique du bloc {bloc} sont : \", specific_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1653a1-6249-4421-8e53-fa6d1e34cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578cc416-883c-466f-8a4e-9c979e75c423",
   "metadata": {},
   "source": [
    "## Comparaison des deux entretiens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211637a8-e7b4-44a3-a5f4-d1d6dcd2c998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gb[\"itw\"] = \"itw_gb\"\n",
    "mr[\"itw\"] = \"itw_mr\"\n",
    "\n",
    "\n",
    "text_gb = [x for x in gb.text]\n",
    "text_mr = [x for x in mr.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea515188-5a31-4275-ae0a-65fafdb7ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tous_documents = []\n",
    "for itw in [text_gb, text_mr]:\n",
    "    lemmatized_an_cleaned_text = lemmatize_and_remove_stopwords(\" \".join(itw), nlp, spacy_stopwords, lemmatiser = True)\n",
    "    tous_documents.append(lemmatized_an_cleaned_text)\n",
    "\n",
    "vectoriseur = TfidfVectorizer(max_df=.65, min_df=1, use_idf=True)\n",
    "documents_transformes = vectoriseur.fit_transform(tous_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8651865-9173-4e19-ae89-020329c66c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_transformes_tableau = documents_transformes.toarray()\n",
    "len(documents_transformes_tableau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb83640-7327-4bf7-89b8-48cbbe9d7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1\n",
    "for compteur, document in enumerate(documents_transformes_tableau):\n",
    "    i+=1\n",
    "    #print(\"#####\\n\",document)\n",
    "    # construire un objet de la classe DataFrame\n",
    "    tf_idf_tuples = list(zip(vectoriseur.get_feature_names_out(), document))\n",
    "    df0 = pd.DataFrame.from_records(tf_idf_tuples, columns=['terme', 'tfidf']).sort_values(by='tfidf', ascending=False).reset_index(drop=True)\n",
    "    df0 = df0.loc[df0.tfidf >= 0]\n",
    "    df0['bloc_id'] = str(compteur)\n",
    "    if i == 0:\n",
    "        df = df0.copy()\n",
    "    else:\n",
    "        df = pd.concat([df, df0])\n",
    "\n",
    "for bloc in df.bloc_id.unique():\n",
    "    df_bloc = df.loc[df.bloc_id == bloc].sort_values(\"tfidf\", ascending=False)\n",
    "    specific_term = [x for x in df_bloc.terme.tail(20)]\n",
    "    print(f\"Les 5 termes les plus spécifique du bloc {bloc} sont : \", specific_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f7dc86-06b8-4de3-86ba-2c84434a3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.terme.str.contains(r\"participat\")]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
